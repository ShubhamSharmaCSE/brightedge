# BrightEdge Crawler Environment Configuration

# Application Settings
APP_NAME=BrightEdge Crawler API
VERSION=1.0.0
ENVIRONMENT=development
DEBUG=true

# Server Settings
HOST=0.0.0.0
PORT=8000

# Database Settings
DATABASE_URL=postgresql://crawler:password@postgres:5432/brightedge
DATABASE_ECHO=false
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20

# Redis Settings
REDIS_URL=redis://redis:6379
REDIS_CACHE_TTL=3600

# AWS Settings (for production)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_SQS_QUEUE_URL=
AWS_S3_BUCKET=

# Crawler Settings
DEFAULT_USER_AGENT=BrightEdge-Crawler/1.0 (+https://github.com/ShubhamSharmaCSE/brightedge)
MAX_CONCURRENT_REQUESTS=100
DEFAULT_CRAWL_DELAY=1.0
MAX_RETRY_ATTEMPTS=3
REQUEST_TIMEOUT=30
MAX_CONTENT_SIZE=10485760

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100
RATE_LIMIT_BURST_SIZE=10

# Security Settings
SECRET_KEY=your-secret-key-here-change-in-production
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000,http://127.0.0.1:3000,http://127.0.0.1:8000

# Monitoring Settings
SENTRY_DSN=
LOG_LEVEL=INFO

# Content Classification
ENABLE_TOPIC_CLASSIFICATION=true
MIN_TOPIC_CONFIDENCE=0.5
MAX_TOPICS_PER_PAGE=10

# Robots.txt Settings
RESPECT_ROBOTS_TXT=true
ROBOTS_TXT_CACHE_TTL=86400
